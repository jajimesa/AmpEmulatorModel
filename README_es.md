# AmpEmulatorModel ()

Este proyecto acad茅mico, [1], contiene la implementaci贸n de un modelo de inteligencia artificial en Python, desarrollado con PyTorch y PyTorch Lightning, cuya arquitectura est谩 basada en WaveNet [2,3]. Su prop贸sito es la emulaci贸n precisa de equipos anal贸gicos de guitarra el茅ctrica, tales como amplificadores a v谩lvulas y pedales de efectos como *overdrive*, distorsi贸n y compresi贸n. Para su uso en un contexto de plugin, se recomienda usar [AmpEmulatorPlugin](https://github.com/jajimesa/AmpEmulatorPlugin), aunque el formato de exportaci贸n (`.json`) es compatible con otros plugins desarrollados por la comunidad [5,6].
> El sistema est谩 dise帽ado para modelar dispositivos que no introduzcan dependencias temporales en la se帽al [3], es decir, que su comportamiento pueda considerarse instant谩neo respecto al contexto temporal de la se帽al de entrada. Con un buen dataset y una buena elecci贸n del dispositivo a modelar, se puede llegar a obtener errores de tan solo el 0.74%, aunque normalmente el intervalo va del 1-5% de Error-to-Signal-Ratio (ESR) [1].

#### Cr茅ditos
Desarrollado originalmente como parte de mi [Trabajo Fin de Grado](https://zenodo.org/records/15490785) para la titulaci贸n de Ingenier铆a Inform谩tica de la Universidad de La Rioja (Espa帽a), titulado *Aprendizaje profundo para la emulaci贸n en tiempo real de equipamiento de guitarra el茅ctrica con Raspberry Pi 5* [1]. Basado conceptualmente en el repositorio [PedalNetRT](https://github.com/GuitarML/PedalNetRT), de Keith Bloemer [4]. Agradezco mucho sus contribuciones a la comunidad, gracias a las cuales no solo he aprendido mucho, sino que he podido testear mi proyecto empleando sus plugins [4].

## Estructura del repositorio

### Carpeta `model/`
Contiene los scripts principales para entrenar, evaluar y exportar modelos de emulaci贸n. 
> En la carpeta `model/data` se almacena el `dataset` que queramos utilizar, mientras que en `model/results`, se guardan los ficheros resultantes de entrenar, exportar e inferir con el modelo. Por 煤ltimo, en `model/tests` se guardan los tests obtenidos.

#### Scripts ejecutables

- `train.py`: entrena un nuevo modelo o retoma el entrenamiento de uno existente.

- `predict.py`: genera una predicci贸n a partir de los datos de entrenamiento limpios usando un modelo entrenado.

- `test.py`: realiza inferencias y genera gr谩ficas para evaluar el rendimiento del modelo.

- `export.py`: convierte un modelo `.ckpt` entrenado en un archivo `.json`, compatible con WaveNetVA o implementaciones en tiempo real.

#### Scripts auxiliares

- `model.py`: define la arquitectura WaveNet en PyTorch Lightning y sus componentes (forward, loss, optimizador...).

- `data.py`: implementa la carga y preparaci贸n de los datos de entrenamiento y validaci贸n. No se ejecuta directamente.

### Carpeta `profiles/`

Contiene perfiles emulados, cada uno en su propia carpeta. Incluyen los datos originales de entrenamiento (guitarra en limpio, `input.wav` y guitarra con el sonido objetivo a replicar `output.wav`), las predicciones generadas y los resultados de los tests asociados al modelo entrenado.

## Instalaci贸n
1. Clona el repositorio, por ejemplo, desde la terminal:
   ```bash
   git clone https://github.com/jajimesa/AmpEmulatorModel.git
   ```
2. Instala las dependencias (se recomienda crear un entorno virtual):
   ```bash
	 pip install -r requirements.txt
   ```

## Gu铆a de uso
Crear un nuevo perfil con el modelo consta de tres pasos: crear el `dataset` > entrenar el modelo > testear el modelo.

#### Preparaci贸n del `dataset`  
El modelo solo requiere aproximadamente de 3 minutos de audio para lograr resultados de alta fidelidad. El dataset consta de dos ficheros, `input.wav` y `output.wav`, ambos con la misma duraci贸n. El primero es una grabaci贸n en limpio de nuestra guitarra, y el segundo es la misma grabaci贸n del primero, pero procesada a trav茅s del amplificador o pedal que queremos simular.
- Graba aproximadamente 3 minutos de audio en formato `.wav` de tu guitarra el茅ctrica en un canal limpio, empleando el mayor n煤mero de t茅cnicas posible y explorando todo el registro tonal de la guitarra. Para mejores resultados, emplea una sola 	pastilla de la guitarra y no toques los potenci贸metros de volumen ni tono. Llama a este fichero `input.wav` y gu谩rdalo en `model/data`. Si no quieres grabar tu propio dataset desde cero, puedes emplear el fichero llamado `input.wav` que se encuentra por defecto en `model/data`.
- Procesa el fichero `input.wav` mediante el amplificador o pedal que quieras emular, y gu谩rdalo en formato `.wav` con el nombre `output.wav`. Si quieres modelar un dispositivo real, se recomienda conectar la salida de la interfaz de audio del ordenador al amplificador o pedal, y recoger su salida a trav茅s de la entrada de la interfaz. Entonces, reproducir el fichero `input.wav` y grabar el resultado.

<img src="data-config.png" width="334" height="448">

> Los archivos `.wav` utilizados en el dataset deben cumplir con las siguientes especificaciones:
> - **Formato**: WAV.
> - **Frecuencia de muestreo**: 44.1 kHz.
> - **Profundidad de bits**: 32-bit FP (punto flotante).
> - **Canales**: Mono.
> - **Duraci贸n**: Aproximadamente 3 minutos.
>   
> Aseg煤rate de que los archivos `.wav` cumplan con estas especificaciones para garantizar la compatibilidad con el modelo de emulaci贸n. La imagen muestra c贸mo lograr esta configuraci贸n empleando el DAW [Reaper](https://www.reaper.fm/).

#### Entrenamiento del `model.ckpt`
Ejecutamos el script `model/train.py`
```bash
python model/train.py
```
#### Testeo del modelo
Invocamos a las utilidades de testing mediante el script `model/test.py`, cuyo output se almacena en `model/tests`:
```bash
python model/test.py
```
Si queremos escuchar el resultado de la inferencia procesando el fichero `data/input.wav` al completo, podemos usar `model/predict.py` y comparar nosotros mismos con el sonido objetivo de `data/output.wav`. El archivo resultante se almacena en `model/results`.

<img src="test-example-1.png" width="575" height="142">
<img src="test-example-2.png" width="575" height="142">	

> Comparaci贸n entre la se帽al real y la predicha, con dos gr谩ficos diferentes, para unos pedales *fuzz face* y *klon centaur*, respectivamente. Se logran erroes min煤sculos, del 0,74% y 0.80%.

#### Exportaci贸n como `model.json`
Podemos exportar el modelo resultante a formato `.json`, para poder ser utilizado en [AmpEmulatorPlugin](https://github.com/jajimesa/AmpEmulatorPlugin), [WaveNetVA](https://github.com/damskaggep/WaveNetVA) o [PedalNetRT](https://github.com/GuitarML/PedalNetRT) en tiempo real.

## Referencias
[1] 	Jim茅nez Santana, J.: "Aprendizaje profundo para la emulaci贸n en tiempo real de equipamiento de guitarra el茅ctrica con Raspberry Pi 5". *Zenodo* (2025). [DOI](https://doi.org/10.5281/zenodo.15490785)   
[2]	Wright, A. et al.: "Real-Time Guitar Amplifier Emulation with Deep Learning". *Applied Sciences* (2020). [DOI](https://doi.org/10.3390/app10030766)   
[3]	Van den Oord, A. et al.: "WaveNet: A Generative Model for Raw Audio". *arXiv* (2016). [DOI](https://doi.org/10.48550/arXiv.1609.03499)   
[4]	Bloemer, K.: PedalNetRT, [GitHub](https://github.com/GuitarML/PedalNetRT). (2020).   
[5]	Damsk盲gg, E.-P.: WaveNetVA, [GitHub](https://github.com/damskaggep/WaveNetVA). (2019).   
[6]	Bloemer, K.: SmartPluginAmp, [GitHub](https://github.com/GuitarML/SmartGuitarAmp). (2020).

Autor: *Javier Jim茅nez Santana*   
Tutores: *Jose Divas贸n Mallagaray, Silvano N谩jera Canal*
